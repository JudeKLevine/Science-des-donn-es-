{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PssRDat3KfmI"
      },
      "source": [
        "# Modélisation probabiliste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_utIcrPcxTq9"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "np.set_printoptions(precision=2,suppress=True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzOKD29tKW7N"
      },
      "source": [
        "## Les limites de la modélisation avec une fonction cible\n",
        "\n",
        "Nous présentons des exemples qui nous montrent les limitations des techniques d'apprentissage \"déterministe\" que l'on a vu précédemment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL2ODlhrKjwy"
      },
      "source": [
        "### Densité discrète ou continue\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpUqmXreXZ4T"
      },
      "source": [
        "\n",
        "### Densité discrète ou continue\n",
        "\n",
        "La densité (ou vraisemblance) d'une variable    $Y$  est une fonction $L(y)$ qui vérifie :  \n",
        "     \n",
        "* Quand $Y\\in \\mathbb R^p$ est continue :\n",
        "$$\n",
        "\\forall \\phi \\qquad \\mathbf E[\\phi(Y)] = \\int_{\\mathbb R^p}  \\phi(y) L(y) \\, dy\n",
        "$$\n",
        "Que l'on peut aussi écrire\n",
        "$$\n",
        "\\mathbf P[Y\\in  dy] = L(y)\\,  dy\n",
        "$$\n",
        "* Quand $Y\\in E$ est discret :\n",
        "$$\n",
        "\\forall \\phi \\qquad  \\mathbf E[\\phi(Y)] = \\sum_{y\\in E}  \\phi(y) L(y)   \n",
        "$$\n",
        "Que l'on peut aussi écrire\n",
        "$$\n",
        "\\mathbf P[Y = y] = L(y)\n",
        "$$\n",
        "     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E70EFGZnXsn-"
      },
      "source": [
        "\n",
        "### Variation sur le bruit\n",
        "\n",
        "Considérons ces deux jeux de données. Les $X_i$ étant en abscisse, et les $Y_i$ en ordonnées.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/vincentvigon/public/blob/main/data/petitSigma.png?raw=true\" width=400><img src=\"https://github.com/vincentvigon/public/blob/main/data/grandSigma.png?raw=true\" width=400>\n",
        "\n",
        "\n",
        "Dans les deux cas, il existe certainement un lien du type $Y=f^?(X)+Bruit$, et sans doute $f^?$ est une fonction affine. Ainsi, on choisit comme modèle :\n",
        "$$\n",
        "f_w(x) = w_0 + w_1 x\n",
        "$$\n",
        "Les techniques du cours précédent nous permettraient de trouver le meilleur couple $(\\hat w_0,\\hat w_1)$.   A vu d'œil, ce  couple serait le même pour deux jeux de données ci-dessus. Cependant, on voit qu'il y a une différence : le premier jeu de donnée est plus bruité que le second.  \n",
        "\n",
        "Il est souvent intéressant de mesurer la variance du bruit ; cela quantifie la qualité des observations.\n",
        "\n",
        "\n",
        "Pour cela, au lieu d'estimer un lien déterministe entre $X$ et $Y$,  on va  plutôt estimer la loi de $Y$ sachant $X=x$. Par exemple, on peut parier que :\n",
        "$$\n",
        "Y =   w_0 + w_1 X  +  \\sigma \\epsilon   \\qquad \\text{avec $\\epsilon  \\sim  \\mathcal N(0,1)$ et $\\sigma>0$}\n",
        "$$\n",
        "Autrement dit, la densité de $Y$ sachant $X=x$ est donnée par\n",
        "$$\n",
        "L_{w,\\sigma}(y|x) = G_{\\sigma}\\Big(y- w_0-w_1x \\Big)      \\qquad           \\text{avec }  G_\\sigma(y)=  \\frac 1 {\\sigma \\sqrt{2\\pi}}  e^{- \\frac {y^2} {2\\sigma^2}}\n",
        "$$\n",
        "Il reste à trouver les meilleurs $w$ et $\\sigma$. A suivre.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7YI96FFX7F-"
      },
      "source": [
        "\n",
        "### Variable caché\n",
        "\n",
        "Considérons  $(X,Y)$, avec $X$ surface d'un appartement et $Y$ prix d'un appartement.\n",
        "Nos observations sont les suivantes~:\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/vincentvigon/public/blob/main/data/troisApparts.png?raw=true\" width=600>\n",
        "\n",
        "Question  :  Qu'est-ce qui pourrait expliquer de telles variations sur les prix ? L'appartenance à un quartier bien sûr !\n",
        "\n",
        "Pour de telles données, on ne peut pas imaginer de fonction $f^?$ telle que $Y=f^?(X) + Bruit$. On pourrait par contre imaginer une fonction telle que $Y=f^?(X,Q)+Bruit$ où $Q$ est  le quartier, mais on ne nous nous a pas donnée $Q$ (on parle de variable cachée).\n",
        "\n",
        "\n",
        "Donc avec les données dont on dispose, peut modéliser $Y$ ainsi :  \n",
        "$$\n",
        "Y =\n",
        "\\begin{cases}\n",
        "w_{0}^0 +w_1^0 X + \\sigma \\epsilon \\qquad &\\text{avec proba } \\frac 1 3\\\\\n",
        "w_{0}^1 +w_1^1 X + \\sigma \\epsilon \\qquad &\\text{avec proba } \\frac 1 3\\\\\n",
        "w_{0}^2 +w_1^2 X + \\sigma \\epsilon \\qquad &\\text{avec proba } \\frac 1 3\n",
        "\\end{cases}\n",
        "$$\n",
        "Ainsi la densité de $Y$ sachant $X=x$ serait :\n",
        "$$\n",
        "L_{w,\\sigma}(y|x) =  \\frac 1 3   G_{\\sigma} ( y - w_0^0 - w_1^0 x   )  +   \\frac 1 3   G_{\\sigma} ( y - w_0^1 - w_1^1 x   )   +  \\frac 1 3   G_{\\sigma} ( y - w_0^2 - w_1^2 x   )    \n",
        "$$\n",
        "Les paramètre inconnus sont ici $\\sigma$et la matrice $w$ de taille $[2,3]$.\n",
        "\n",
        "Remarques : puisqu'on ne nous a pas fournis une variable visiblement très importante (le quartier des appartements), on a mis cette variable cachée en paramètre inconnu (c'est l'exposant dans le tenseur $w$). On demande à l'algorithme d'optimisation de faire au mieux avec ces inconnues supplémentaires : le meilleur couple de paramétre sera appelé $\\hat w,\\hat\\sigma$, et la densité estimée sera $\\hat L = L_{\\hat w,\\hat\\sigma}$.\n",
        "\n",
        "\n",
        "Bien entendu, on pourrait augmenter la flexibilité du modèle en supposons que le nombre d'appartements par quartier n'est pas le même, ou en supposant que le bruit par quartier n'est pas le même.  Mais il faut que cela soit vraiment nécessaire, car lorsqu'on a trop d'inconnue ...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl11MhTHYGxF"
      },
      "source": [
        "\n",
        "### Et si on veut quand même faire de la prédiction\n",
        "\n",
        "Ainsi, notre nouvel objectif est de trouver un $\\hat L(y|x)$ qui décrive au mieux la loi de $Y$ sachant $X=x$.\n",
        "Ensuite, on peut poser $\\hat f(x) = \\text{argmax}_y \\hat L(y|x) $.   \n",
        "\n",
        "Cette technique  repose  sur l'hypothèse que l'endroit le plus probable d'apparition d'une v.a., c'est l'argmax de sa densité. Mais ce n'est pas vrai en général :\n",
        "\n",
        "<img src=\"https://github.com/vincentvigon/public/blob/main/data/maximumDensite.png?raw=true\" width=400>\n",
        "\n",
        "\n",
        "Heureusement, dans la nature, les densités ont des formes bien plus sages (ex: des gaussiennes).\n",
        "\n",
        "\n",
        "Autre manière d'estimer: on peut  prendre l'espérance:\n",
        "$$\n",
        "\\hat f(x) = \\int y \\, \\hat L(y|x)\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EQi2OXrYMv5"
      },
      "source": [
        "\n",
        "## Distance cross-entropique\n",
        "\n",
        "\n",
        "### Trouver la meilleure densité via une distance\n",
        "\n",
        "\n",
        "Notre problème est donc de trouver une densité conditionnelle  ou vraisemblance   $\\hat L(y|x)$ qui représente au mieux la distribution de $Y$ sachant $X=x$. On va rechercher cette densité dans une famille paramétrique  $\\mathcal L=\\{L_\\theta : \\theta \\in \\Theta \\}$.\n",
        "\n",
        "Pour trouver le meilleur élément de cette famille on peut prendre:\n",
        "$$\n",
        "\\hat \\theta =  \\text{argmin} _{\\theta\\in \\Theta}    \\sum_{Train}  \\text{dist} \\Big( y_i,L_\\theta ( \\cdot | x_i  )     \\Big )\n",
        "$$\n",
        "puis $\\hat L = L_{\\hat \\theta}$.\n",
        "\n",
        "\n",
        "Attention :    $\\text{dist}$ est une distance entre une densité  $L$ et une observation $y_i$. La distance sera petite quand la densité charge beaucoup l'observation.\n",
        "\n",
        "\n",
        "La distance la plus connue, pour le cas discret et continu est\n",
        "$$\n",
        "\\text{dist}   \\Big(  L  ,   y    \\Big ) =H   \\Big(    y ,  L   \\Big )   = - \\ln L(y)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "owFESdetxQFB",
        "cellView": "form",
        "outputId": "2bce118c-799d-4093-a1e7-3f2e412c99ea"
      },
      "source": [
        "#@title Cross Entropie\n",
        "a=0.15\n",
        "b=0.6\n",
        "x=np.linspace(0.1,1,100)\n",
        "L=lambda x : stats.beta.pdf(x,3,2)\n",
        "_log_L=lambda x:-np.log(L(x))\n",
        "plt.plot(x,L(x),label=\"L\")\n",
        "plt.plot(x,_log_L(x),label=\"-log(L)\")\n",
        "plt.plot([a,a],[0,_log_L(a)],\"k\")\n",
        "plt.plot([b,b],[0,_log_L(b)],\"k\")\n",
        "plt.plot(x,np.zeros_like(x),\"k\")\n",
        "ep=0.01\n",
        "plt.text(a+ep, 3*ep, \"a\", fontsize=12)\n",
        "plt.text(b+ep, 3*ep, \"b\", fontsize=12)\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCWzK3oK0pk5"
      },
      "source": [
        "ci-dessus:\n",
        "* $H(a,L)$ est grande: la densité $L$ est \"éloignée\" de $\\delta_a$.\n",
        "* $H(b,L)$ est petite (et même négative): la densité $L$ est \"proche\" de $\\delta_b$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7Q4DSiYYcHv"
      },
      "source": [
        "\n",
        "### Cas discret\n",
        "\n",
        "\n",
        "Dans le cas discret, quand $p$ et $q$ sont des probas, on écrit  généralement\n",
        "$$\n",
        "H  ( p,q ) =  -  \\sum_v   p(v)   \\ln q (v)\n",
        "$$\n",
        "Ainsi en notant $\\delta_y$ la Dirac en $y$:\n",
        "$$\n",
        "H  (  y , L  ) =     H (  \\delta_y,  L  )=  - \\sum_v \\delta_y(v)   \\ln L(v)  =-\\log L(y)  \n",
        "$$\n",
        "Attention, il y a deux notions proches: la divergence de Kullback–Leibler:\n",
        "$$\n",
        "D_{KL} ( p || q )  =  - \\sum_v    p (v) \\ln \\frac{q(v)}{p(v)}  \n",
        "$$\n",
        "et l'entropie:\n",
        "$$\n",
        "H(p) = - \\sum_v   p(v)  \\ln p (v)\n",
        "$$\n",
        "qui sont reliés par\n",
        "$$\n",
        "H(p,q) = H(p) + D_{KL} ( p || q )\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdS0ZgiEYkhd"
      },
      "source": [
        "\n",
        "### Construire $\\hat L$ par maximum de vraisemblance\n",
        "\n",
        "L'indépendance des observations, nous indique que la densité d'un échantillon $(Y_1,...,Y_n)$  sachant  $X_1=x_1,...,X_n=x_n$ est :\n",
        "$$\n",
        " (y_1,...,y_n) \\to  \\prod_{Train}  L_{\\theta} (y_i | x_i)\n",
        "$$\n",
        "Ainsi, quand  on observe $Train= [(y_1,x_1), ...,(y_n,x_n)] $, le paramètre $\\theta$ qui rend ces observations les plus vraisemblables est  :\n",
        "$$\n",
        "\\hat \\theta = \\text{argmax}_{\\theta \\in \\Theta}   \\prod_{Train}  L_{\\theta} (y_i | x_i)\n",
        "$$\n",
        " On doit estimer l'argmax d'un produit.   Comme on préfère les sommes, on passe le tout au logarithme. C'est une fonction croissante, donc :\n",
        "$$\n",
        "\\hat \\theta = \\text{argmax}_{\\theta \\in \\Theta}   \\sum_{ Train}   \\ln L_{\\theta} (y_i | x_i)\n",
        "$$\n",
        "En mettant un signe moins devant :\n",
        "$$\n",
        "\\hat \\theta = \\text{argmin} _{\\theta \\in \\Theta}   \\sum_{Train}    - \\ln L_{\\theta} (y_i | x_i) =\\text{argmin} _{\\theta \\in \\Theta}   \\sum_{ Train} H \\big(y_i ,   L_{\\theta} ( \\cdot  | x_i)      \\big)\n",
        "$$\n",
        "Conclusion: maximiser la vraissemblance ou minimiser la distance cross-entropique, c'est idem.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "970O66XlYuuT"
      },
      "source": [
        "\n",
        "### Exercice\n",
        "\n",
        "Considérez le modèle linéaire gaussien avec le paramètre $\\sigma =1$ :\n",
        "$$\n",
        "L_{w}(y|x) = G\\Big(y- w_0-w_1x \\Big)      \\qquad           \\text{avec }  G(y)=  \\frac 1 {\\sqrt{2\\pi}}  e^{- \\frac {y^2} {2}}\n",
        "$$\n",
        "Ecrire le problème de minimisation donné par le maximum de vraissemblance. Que constatez-vous ?\n",
        "\n",
        "\n",
        "Plus dur: Refaites le même exo quand $\\sigma$est inconnu :\n",
        "$$\n",
        "L_{w,\\sigma}(y|x) = G_\\sigma\\Big(y- w_0-w_1x \\Big)      \\qquad           \\text{avec }  G_\\sigma(y)=  \\frac 1 {\\sigma \\sqrt{2\\pi}}  e^{- \\frac {y^2} {2\\sigma^2}}\n",
        "$$\n",
        "Le problème de minimisation obtenu admet une solution explicite $(\\hat w, \\hat \\sigma)$, que l'on calcule en cherchant le lieu où la différentielle s'annule. Pour $\\hat w$, on tombe sur l'estimateur déjà vu, pour $\\hat \\sigma$on tombe sur une formule naturelle qui est implémentée dans la classe du TP.  Si on n'aime pas calculer la différentielle, on peut aussi demander un algo d'optimisation de  trouver $(\\hat w, \\hat \\sigma)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxA1mxI2ZBAp"
      },
      "source": [
        "## Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZs_TjlJZDqm"
      },
      "source": [
        "\n",
        "### Construction générique\n",
        "\n",
        "On se spécialise maintenant dans le cas où l'output $Y$ est qualitatif. Pour simplifier, on appellera les classes $\\{1,2,....,k\\}$.\n",
        "\n",
        "Une densité  $L$ sur $\\{1,2,....,k\\}$ est assimilée au vecteur $[L(1),...,L(k)] \\in \\mathbb R^k$, vecteur positif dont la somme fait 1.\n",
        "\n",
        "\n",
        "On considère  les variables explicatives $X=(X^1,...,X^p)$.  \n",
        "\n",
        "\n",
        "Définissons la fonction softmax:\n",
        "$$\n",
        " \\Big ( \\mathtt{SM}     (  V)  \\Big)_y    =   \\frac{ e^{V_y} }{\\sum_u e^{V_u}}\n",
        "$$\n",
        "qui transforme tout vecteur $V$ en un vecteur de probabilité.\n",
        "\n",
        "\n",
        "Pour construire un modèle paramètrique, commence par se donner une famille de fonction $f_\\theta$ de $\\mathbb R^p$ dans $\\mathbb R^k$.  Puis, on définit:\n",
        "$$\n",
        "L_\\theta (y  | x) =  \\Big(\\mathtt{SM} \\circ f_\\theta (x) \\Big)_y\n",
        "$$\n",
        "La quasi-totalité des modèles de classification sont de la forme ci-dessus\n",
        "\n",
        "\n",
        "Exo: caculer numériquement\n",
        "$$\n",
        "\\mathtt{SM}(1,-3,10,1)\n",
        "$$\n",
        "\n",
        "Vocabulaire: le résultat du modèle avant le softmax: $f_\\theta (x)$ est souvent appelé les \"logits\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DH8BFXshtpe"
      },
      "source": [
        "import numpy as np\n",
        "#@title Solution:\n",
        "show=False #@param {type:\"boolean\"}\n",
        "if show:\n",
        "    V=[1,-3,10,1]\n",
        "    print(np.exp(V)/np.sum(np.exp(V)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KbC9uqQZK3J"
      },
      "source": [
        "\n",
        "### Régression logistique (ou softmax)\n",
        "\n",
        "C'est le modèle de classification le plus simple:  le paramètre est $\\theta= (w,b)$ avec $w$ une matrice $p\\times k$ et $b$ un vecteur de taille $k$ et on choisit:\n",
        "$$\n",
        "f_{w,b}(x) = x \\cdot w  + b  \n",
        "$$\n",
        "puis\n",
        "$$\n",
        "L_{w,b} (y  | x) =  \\Big(\\mathtt{SM} \\circ f_{w,b} (x) \\Big)_y\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuyA8CEVZOC3"
      },
      "source": [
        "\n",
        "### Ex: Réseau de neurone\n",
        "\n",
        "Donnons un exemple de réseau de neurone classificateur dense (=fully-connected) à 2 couches (=1 couche d'entrée, 1 couche cachée, 1 couche de sortie).\n",
        "\n",
        "\n",
        "On se donne une fonction non linéaire $\\ell$, par exemple\n",
        "     \n",
        "* $\\ell(x)=x 1_{x>0}$ la fonction relu.\n",
        "* $\\ell(x)=\\tanh(x)$\n",
        "* $\\ell(x)= \\frac{1}{1+e^{-x}}$ la sigmoïde.\n",
        "     \n",
        "\n",
        "Si $V$ est un vecteur, on note $\\ell(V)$ l'application de $\\ell$ à toutes les composantes de $V$.\n",
        "\n",
        "On prend comme paramètre  $\\theta= (w,b,w',b')$ composé de deux matrices et de vecteurs. Puis on choisit\n",
        "$$\n",
        "f_{\\theta}(x) =       \\ell (x \\cdot w   + b)\\cdot w' +b'\n",
        "$$\n",
        "puis\n",
        "$$\n",
        "L_{\\theta} (y  | x) =  \\Big(\\mathtt{SM} \\circ f_{\\theta} (x) \\Big)_y\n",
        "$$\n",
        "DESSIN\n",
        "\n",
        "\n",
        "***A vous:*** Et maintenant le réseau suivant a combien de couche?\n",
        "$$\n",
        "f_{\\theta}(x) =   \\ell \\Big(   \\ell (x \\cdot w   + b)\\cdot w' +b' \\Big)\\cdot w'' +b''\n",
        "$$\n",
        "puis\n",
        "$$\n",
        "L_{\\theta} (y  | x) =  \\Big(\\mathtt{SM} \\circ f_{\\theta} (x) \\Big)_y\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyplnQq-E93s"
      },
      "source": [
        "Voici une implémentation du réseau à une couche.\n",
        "\n",
        "***À vous:*** Implémenter le réseau plus complexe. Pour les dimensions qui ne sont pas précisées, choisissez!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vbK4L47C7QS",
        "outputId": "2310a196-acb8-4d2d-9ed6-7cda04901985"
      },
      "source": [
        "import numpy as np\n",
        "p=5\n",
        "k=2\n",
        "w=np.random.normal(size=[p,k])\n",
        "b=np.ones(shape=[k])\n",
        "ell=np.tanh\n",
        "\n",
        "def f_theta(x):\n",
        "    return ell(x@w+b)\n",
        "\n",
        "x=np.ones([p])\n",
        "f_theta(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.62709948, 0.93831214])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiO4j_OlZRJ2"
      },
      "source": [
        "\n",
        "### Classifier\n",
        "\n",
        "\n",
        "Ça y est: vous avez trouvé une bonne vraisemblance $L_{\\hat w}$ pour décrire vos données.  \n",
        "\n",
        "Maintenant, on vous donne une nouvelle entrée $x_o$. Pour prédire la sortie correspondante, le plus naturel, c'est de prendre la classe qui a la plus grande probabilité:\n",
        "$$\n",
        "y_o = \\text{argmax}_y  L_{\\hat w}(y|x_o)\n",
        "$$\n",
        "\n",
        "\n",
        "Cependant, toutes les classes n'ont pas la même importance. Imaginons que nous cherchons à repérer des malades en vue de les soigner.  La classe $1$ (ou positif)  étant \"Malade\", et la classe $0$ (ou négatif) étant \"sain\".  Supposons que  \n",
        "$$\n",
        "L_{\\hat w}( \\cdot | x_o)= [ 0.52 , 0.48    ]\n",
        "$$\n",
        "Dans ce cas-là, il vaut  peut-être mieux classer le sujet \"malade\" pour le soigner par précaution.\n",
        "\n",
        "\n",
        "En classification binaire (2 classes),  on a inventé de  nombreux outils pour choisir le bon seuil de probabilité: courbe ROC, score AUC, score F1.\n",
        "\n",
        "En classification multi-classe, on analyse surtout la matrice de confusion.\n",
        "\n",
        "On verra tout cela dans les T.P.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcqZqdL4ZXZx"
      },
      "source": [
        "\n",
        "### Exo\n",
        "\n",
        "\n",
        "Dans cette partie, nous nous plaçons dans le cas le plus simple où il y a deux descripteurs $p=2$ et deux classes $k=2$. On considère un modèle logistique  que l'on a entrainée avec des données $Train$.  On a obtenu des paramètres $(\\hat w,\\hat b)$ optimaux.\n",
        "\n",
        "On choisit ensuite de classifier un individu par la méthode la plus simple:\n",
        "$$\n",
        "\\hat y_o=1    \\Leftrightarrow L_{\\hat w}(1  | x_o) > L_{\\hat w}(0  | x_o) \\Leftrightarrow L_{\\hat w}(1  | x_o) >0.5\n",
        "$$\n",
        "À quoi ressemble la frontière de décision, c.à-d. la limite entre les $x\\in \\mathbb R^2$ dont la classe prédite est 0 et ceux dont la classe prédite est 1.\n",
        "\n",
        "Même question si maintenant $k=3$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhW2RqG0TFZU"
      },
      "source": [
        "### Correction\n",
        "\n",
        "$$\n",
        "L(\\cdot |x) = \\mathtt{SM}(x\\cdot w + b )\n",
        "$$\n",
        "Donc\n",
        "$$\n",
        "L(0 |x) = e^{(x\\cdot w + b )_0} /cst\n",
        "$$\n",
        "$$\n",
        "L(1 |x) = e^{(x\\cdot w + b )_1} /cst\n",
        "$$\n",
        "Donc\n",
        "$$\n",
        "L(0 |x)>L(1 |x) \\Leftrightarrow (x\\cdot w + b )_0>(x\\cdot w + b )_1\n",
        "$$\n",
        "Les applications:\n",
        "\\begin{align}\n",
        "\\mathbb R^2 &\\to \\mathbb R\\\\\n",
        "x &\\mapsto \\mathbb (x\\cdot w + b )_0\\\\\n",
        "x &\\mapsto \\mathbb (x\\cdot w + b )_1\\\\\n",
        "\\end{align}\n",
        "se représente par des plans. Le lieu des $x$ où la première domine la seconde est donc un demi-espace dont la frontière est une droite.\n",
        "\n",
        "\n",
        "Si on a 3 classes possibles, on a donc la concurence entre 3 plans\n",
        "\\begin{align}\n",
        "\\mathbb R^2 &\\to \\mathbb R\\\\\n",
        "x &\\mapsto \\mathbb (x\\cdot w + b )_0\\\\\n",
        "x &\\mapsto \\mathbb (x\\cdot w + b )_1\\\\\n",
        "x &\\mapsto \\mathbb (x\\cdot w + b )_2\\\\\n",
        "\\end{align}\n",
        "Le lieu des $x$ où la première domine est donc typiquement un secteur angulaire (sauf cas très particulier).\n",
        "\n",
        "Quand on a plus de classe, les zones de domination peuvent devnir plus complexe, mais la frontière de décision est toujours constituée de segments de droite.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}